{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50420b4a",
   "metadata": {},
   "source": [
    "## Example from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6fefc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ho2298/anaconda3/envs/openaiclip/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "Top predictions:\n",
      "\n",
      "snake: 65.19%\n",
      "turtle: 12.44%\n",
      "sweet_pepper: 3.85%\n",
      "lizard: 1.88%\n",
      "crocodile: 1.74%\n"
     ]
    }
   ],
   "source": [
    "## Example from OpenAI\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "\n",
    "# Download the dataset\n",
    "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
    "\n",
    "# Prepare the inputs\n",
    "image, class_id = cifar100[3637]\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
    "\n",
    "# Calculate features\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(5)\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{cifar100.classes[index]}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def60410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88c79cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_adverserial_inference_df(inference_path, fgsm = False):\n",
    "    '''\n",
    "    Create dataframe for our inference files\n",
    "    inference_path: path to our inference folder \n",
    "    \n",
    "    retunrs df with image pointing to image filename and caption to the correct label \n",
    "    '''\n",
    "    #file_path_names = [f for f in os.listdir(inference_path) if os.path.isfile(f)]\n",
    "    file_path_names = os.listdir(inference_path)\n",
    "    d = {} \n",
    "    d['image'] = file_path_names\n",
    "    labels = []\n",
    "    captions = []\n",
    "    for file_path in file_path_names:\n",
    "        if fgsm:\n",
    "            name = os.path.splitext(file_path)[0] # remove .JPEG from the end \n",
    "            name = ' '.join(name.split('_'))  # \"01669191_box_turtle\"  to \"box turtle\"\n",
    "            label = name\n",
    "            name = \"This is a photo of a \" + name  # \"box turtle\" to \"This is box turtle\"\n",
    "        else:\n",
    "            name = os.path.splitext(file_path)[0] # remove .JPEG from the end \n",
    "            name = ' '.join(name.split('_')[1:])  # \"01669191_box_turtle\"  to \"box turtle\"\n",
    "            label = name\n",
    "            name = \"This is a photo of a \" + name  # \"box turtle\" to \"This is box turtle\"\n",
    "        captions.append(name)\n",
    "        labels.append(label)\n",
    "    d['caption'] = captions \n",
    "    d['label'] = labels\n",
    "    df = pd.DataFrame(data=d)\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac9d6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match(image_input, text_inputs, model, classes, k=5, print_only=False):\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    # Pick the top 5 most similar labels for the image\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity[0].topk(k)\n",
    "    if print_only:\n",
    "        for value, index in zip(values, indices):\n",
    "            print(f\"{classes[index]:>16s}: {100 * value.item():.2f}%\")\n",
    "    matches = [classes[index] for index in indices]\n",
    "    return matches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da35082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "def top_k_inf(model, df, k=5, attack_mode=None):\n",
    "    \n",
    "    # appending the correct folder path\n",
    "    if attack_mode is None:\n",
    "        source_path = \"data/originalImagenet/\"\n",
    "    elif attack_mode == \"pgd\":\n",
    "        source_path = \"data/pgdAttack/\"\n",
    "    elif attack_mode == \"fgsm\":\n",
    "        source_path = \"data/attack/\"\n",
    "    correct = 0\n",
    "    \n",
    "    text_inputs = df['caption'].values\n",
    "    text_inputs = torch.cat([clip.tokenize(c) for c in text_inputs]).to(device)\n",
    "\n",
    "    for index,row in tqdm(df.iterrows(), total=len(df)): \n",
    "        image_path = source_path + row['image']\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "        matches = find_match(image_input, text_inputs, model, df['caption'].values, k,  False)\n",
    "\n",
    "        for match in matches:\n",
    "            if row['label'] in match: \n",
    "                correct += 1\n",
    "        if index % 100 == 0:\n",
    "            print(f\"Correct raw: {correct}, Percent: {round(correct/(index+1) *100, 2)}\")\n",
    "\n",
    "    print(f\"Attack mode is {attack_mode}\")\n",
    "    print(f\"Correct raw value: {correct}\")\n",
    "    print(f\"Top {k} percent: {round(correct/len(df)* 100, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d56368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No attack - originalImagnet dataset \n",
    "model, preprocess = clip.load('ViT-B/32', device)\n",
    "og_df = make_adverserial_inference_df(\"data/originalImagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ea149b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n02979186_cassette_player.JPEG</td>\n",
       "      <td>This is a photo of a cassette player</td>\n",
       "      <td>cassette player</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n02113799_standard_poodle.JPEG</td>\n",
       "      <td>This is a photo of a standard poodle</td>\n",
       "      <td>standard poodle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n02437312_Arabian_camel.JPEG</td>\n",
       "      <td>This is a photo of a Arabian camel</td>\n",
       "      <td>Arabian camel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n04154565_screwdriver.JPEG</td>\n",
       "      <td>This is a photo of a screwdriver</td>\n",
       "      <td>screwdriver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n12998815_agaric.JPEG</td>\n",
       "      <td>This is a photo of a agaric</td>\n",
       "      <td>agaric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>n01582220_magpie.JPEG</td>\n",
       "      <td>This is a photo of a magpie</td>\n",
       "      <td>magpie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>n03874599_padlock.JPEG</td>\n",
       "      <td>This is a photo of a padlock</td>\n",
       "      <td>padlock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>n03240683_drilling_platform.JPEG</td>\n",
       "      <td>This is a photo of a drilling platform</td>\n",
       "      <td>drilling platform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>n01795545_black_grouse.JPEG</td>\n",
       "      <td>This is a photo of a black grouse</td>\n",
       "      <td>black grouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>n04019541_puck.JPEG</td>\n",
       "      <td>This is a photo of a puck</td>\n",
       "      <td>puck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                image                                 caption  \\\n",
       "0      n02979186_cassette_player.JPEG    This is a photo of a cassette player   \n",
       "1      n02113799_standard_poodle.JPEG    This is a photo of a standard poodle   \n",
       "2        n02437312_Arabian_camel.JPEG      This is a photo of a Arabian camel   \n",
       "3          n04154565_screwdriver.JPEG        This is a photo of a screwdriver   \n",
       "4               n12998815_agaric.JPEG             This is a photo of a agaric   \n",
       "..                                ...                                     ...   \n",
       "995             n01582220_magpie.JPEG             This is a photo of a magpie   \n",
       "996            n03874599_padlock.JPEG            This is a photo of a padlock   \n",
       "997  n03240683_drilling_platform.JPEG  This is a photo of a drilling platform   \n",
       "998       n01795545_black_grouse.JPEG       This is a photo of a black grouse   \n",
       "999               n04019541_puck.JPEG               This is a photo of a puck   \n",
       "\n",
       "                 label  \n",
       "0      cassette player  \n",
       "1      standard poodle  \n",
       "2        Arabian camel  \n",
       "3          screwdriver  \n",
       "4               agaric  \n",
       "..                 ...  \n",
       "995             magpie  \n",
       "996            padlock  \n",
       "997  drilling platform  \n",
       "998       black grouse  \n",
       "999               puck  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc1c77d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 0, Percent: 0.0\n",
      "Correct raw: 9, Percent: 81.82\n",
      "Correct raw: 19, Percent: 90.48\n",
      "Correct raw: 28, Percent: 90.32\n",
      "Correct raw: 38, Percent: 92.68\n",
      "Correct raw: 46, Percent: 90.2\n",
      "Correct raw: 55, Percent: 90.16\n",
      "Correct raw: 62, Percent: 87.32\n",
      "Correct raw: 70, Percent: 86.42\n",
      "Correct raw: 78, Percent: 85.71\n",
      "Correct raw: 88, Percent: 87.13\n",
      "Correct raw: 98, Percent: 88.29\n",
      "Correct raw: 106, Percent: 87.6\n",
      "Correct raw: 118, Percent: 90.08\n",
      "Correct raw: 126, Percent: 89.36\n",
      "Correct raw: 136, Percent: 90.07\n",
      "Correct raw: 145, Percent: 90.06\n",
      "Correct raw: 154, Percent: 90.06\n",
      "Correct raw: 164, Percent: 90.61\n",
      "Correct raw: 174, Percent: 91.1\n",
      "Correct raw: 183, Percent: 91.04\n",
      "Correct raw: 192, Percent: 91.0\n",
      "Correct raw: 202, Percent: 91.4\n",
      "Correct raw: 211, Percent: 91.34\n",
      "Correct raw: 218, Percent: 90.46\n",
      "Correct raw: 228, Percent: 90.84\n",
      "Correct raw: 237, Percent: 90.8\n",
      "Correct raw: 247, Percent: 91.14\n",
      "Correct raw: 256, Percent: 91.1\n",
      "Correct raw: 263, Percent: 90.38\n",
      "Correct raw: 268, Percent: 89.04\n",
      "Correct raw: 277, Percent: 89.07\n",
      "Correct raw: 287, Percent: 89.41\n",
      "Correct raw: 295, Percent: 89.12\n",
      "Correct raw: 306, Percent: 89.74\n",
      "Correct raw: 314, Percent: 89.46\n",
      "Correct raw: 321, Percent: 88.92\n",
      "Correct raw: 330, Percent: 88.95\n",
      "Correct raw: 339, Percent: 88.98\n",
      "Correct raw: 350, Percent: 89.51\n",
      "Correct raw: 358, Percent: 89.28\n",
      "Correct raw: 366, Percent: 89.05\n",
      "Correct raw: 375, Percent: 89.07\n",
      "Correct raw: 385, Percent: 89.33\n",
      "Correct raw: 393, Percent: 89.12\n",
      "Correct raw: 399, Percent: 88.47\n",
      "Correct raw: 407, Percent: 88.29\n",
      "Correct raw: 417, Percent: 88.54\n",
      "Correct raw: 426, Percent: 88.57\n",
      "Correct raw: 434, Percent: 88.39\n",
      "Correct raw: 441, Percent: 88.02\n",
      "Correct raw: 452, Percent: 88.45\n",
      "Correct raw: 462, Percent: 88.68\n",
      "Correct raw: 469, Percent: 88.32\n",
      "Correct raw: 476, Percent: 87.99\n",
      "Correct raw: 486, Percent: 88.2\n",
      "Correct raw: 494, Percent: 88.06\n",
      "Correct raw: 503, Percent: 88.09\n",
      "Correct raw: 512, Percent: 88.12\n",
      "Correct raw: 522, Percent: 88.32\n",
      "Correct raw: 532, Percent: 88.52\n",
      "Correct raw: 538, Percent: 88.05\n",
      "Correct raw: 546, Percent: 87.92\n",
      "Correct raw: 555, Percent: 87.96\n",
      "Correct raw: 564, Percent: 87.99\n",
      "Correct raw: 573, Percent: 88.02\n",
      "Correct raw: 581, Percent: 87.9\n",
      "Correct raw: 588, Percent: 87.63\n",
      "Correct raw: 597, Percent: 87.67\n",
      "Correct raw: 606, Percent: 87.7\n",
      "Correct raw: 615, Percent: 87.73\n",
      "Correct raw: 626, Percent: 88.05\n",
      "Correct raw: 634, Percent: 87.93\n",
      "Correct raw: 641, Percent: 87.69\n",
      "Correct raw: 649, Percent: 87.58\n",
      "Correct raw: 658, Percent: 87.62\n",
      "Correct raw: 667, Percent: 87.65\n",
      "Correct raw: 673, Percent: 87.29\n",
      "Correct raw: 683, Percent: 87.45\n",
      "Correct raw: 692, Percent: 87.48\n",
      "Correct raw: 700, Percent: 87.39\n",
      "Correct raw: 708, Percent: 87.3\n",
      "Correct raw: 715, Percent: 87.09\n",
      "Correct raw: 725, Percent: 87.24\n",
      "Correct raw: 734, Percent: 87.28\n",
      "Correct raw: 743, Percent: 87.31\n",
      "Correct raw: 753, Percent: 87.46\n",
      "Correct raw: 763, Percent: 87.6\n",
      "Correct raw: 771, Percent: 87.51\n",
      "Correct raw: 781, Percent: 87.65\n",
      "Correct raw: 788, Percent: 87.46\n",
      "Correct raw: 795, Percent: 87.27\n",
      "Correct raw: 803, Percent: 87.19\n",
      "Correct raw: 813, Percent: 87.33\n",
      "Correct raw: 821, Percent: 87.25\n",
      "Correct raw: 828, Percent: 87.07\n",
      "Correct raw: 837, Percent: 87.1\n",
      "Correct raw: 845, Percent: 87.02\n",
      "Correct raw: 856, Percent: 87.26\n",
      "Correct raw: 864, Percent: 87.18\n",
      "Attack mode is None\n",
      "Correct raw value: 872\n",
      "Top 5 percent: 87.2\n"
     ]
    }
   ],
   "source": [
    "top_k_inf(model, og_df, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4fd324f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_df = make_adverserial_inference_df(\"data/pgdAttack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52257c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n02979186_cassette_player.JPEG</td>\n",
       "      <td>This is a photo of a cassette player</td>\n",
       "      <td>cassette player</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n02113799_standard_poodle.JPEG</td>\n",
       "      <td>This is a photo of a standard poodle</td>\n",
       "      <td>standard poodle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n02437312_Arabian_camel.JPEG</td>\n",
       "      <td>This is a photo of a Arabian camel</td>\n",
       "      <td>Arabian camel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n04154565_screwdriver.JPEG</td>\n",
       "      <td>This is a photo of a screwdriver</td>\n",
       "      <td>screwdriver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n12998815_agaric.JPEG</td>\n",
       "      <td>This is a photo of a agaric</td>\n",
       "      <td>agaric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>n01582220_magpie.JPEG</td>\n",
       "      <td>This is a photo of a magpie</td>\n",
       "      <td>magpie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>n03874599_padlock.JPEG</td>\n",
       "      <td>This is a photo of a padlock</td>\n",
       "      <td>padlock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>n03240683_drilling_platform.JPEG</td>\n",
       "      <td>This is a photo of a drilling platform</td>\n",
       "      <td>drilling platform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>n01795545_black_grouse.JPEG</td>\n",
       "      <td>This is a photo of a black grouse</td>\n",
       "      <td>black grouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>n04019541_puck.JPEG</td>\n",
       "      <td>This is a photo of a puck</td>\n",
       "      <td>puck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                image                                 caption  \\\n",
       "0      n02979186_cassette_player.JPEG    This is a photo of a cassette player   \n",
       "1      n02113799_standard_poodle.JPEG    This is a photo of a standard poodle   \n",
       "2        n02437312_Arabian_camel.JPEG      This is a photo of a Arabian camel   \n",
       "3          n04154565_screwdriver.JPEG        This is a photo of a screwdriver   \n",
       "4               n12998815_agaric.JPEG             This is a photo of a agaric   \n",
       "..                                ...                                     ...   \n",
       "995             n01582220_magpie.JPEG             This is a photo of a magpie   \n",
       "996            n03874599_padlock.JPEG            This is a photo of a padlock   \n",
       "997  n03240683_drilling_platform.JPEG  This is a photo of a drilling platform   \n",
       "998       n01795545_black_grouse.JPEG       This is a photo of a black grouse   \n",
       "999               n04019541_puck.JPEG               This is a photo of a puck   \n",
       "\n",
       "                 label  \n",
       "0      cassette player  \n",
       "1      standard poodle  \n",
       "2        Arabian camel  \n",
       "3          screwdriver  \n",
       "4               agaric  \n",
       "..                 ...  \n",
       "995             magpie  \n",
       "996            padlock  \n",
       "997  drilling platform  \n",
       "998       black grouse  \n",
       "999               puck  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d67b9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                                             | 1/1000 [00:01<21:13,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 0, Percent: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████████▏                                                                                                                             | 101/1000 [02:06<18:51,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 68, Percent: 67.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████████████████▏                                                                                                               | 201/1000 [04:12<16:45,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 140, Percent: 69.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████████████████████████████████████▏                                                                                                 | 301/1000 [06:18<14:41,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 204, Percent: 67.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████▏                                                                                   | 401/1000 [08:24<12:34,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 262, Percent: 65.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████▏                                                                     | 501/1000 [10:30<10:28,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 320, Percent: 63.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████████▏                                                       | 601/1000 [12:36<08:22,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 389, Percent: 64.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████████████████████████████▏                                         | 701/1000 [14:42<06:16,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 447, Percent: 63.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                           | 801/1000 [16:48<04:10,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 501, Percent: 62.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏             | 901/1000 [18:54<02:04,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 566, Percent: 62.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [20:59<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack mode is pgd\n",
      "Correct raw value: 622\n",
      "Top 5 percent: 62.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top_k_inf(model, pgd_df, k=5, attack_mode='pgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47601605",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm_df = make_adverserial_inference_df(\"data/attack\", fgsm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e3a880f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                                              | 1/997 [00:01<21:02,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 0, Percent: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████████▎                                                                                                                              | 101/997 [02:06<18:47,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 44, Percent: 43.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████████████████▍                                                                                                                | 201/997 [04:12<16:42,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 93, Percent: 46.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████████████████████████████████████▌                                                                                                  | 301/997 [06:18<14:36,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 148, Percent: 49.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████▋                                                                                    | 401/997 [08:24<12:29,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 193, Percent: 48.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████▊                                                                      | 501/997 [10:30<10:23,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 237, Percent: 47.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████████▉                                                        | 601/997 [12:35<08:17,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 276, Percent: 45.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████████████████████████████████████████████████████████████████████████████████████▏                                         | 701/997 [14:41<06:12,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 319, Percent: 45.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                           | 801/997 [16:47<04:06,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 359, Percent: 44.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍             | 901/997 [18:53<02:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 413, Percent: 45.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 997/997 [20:54<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack mode is fgsm\n",
      "Correct raw value: 462\n",
      "Top 5 percent: 46.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top_k_inf(model, fgsm_df, k=5, attack_mode='fgsm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41839336",
   "metadata": {},
   "source": [
    "### Loading Fine-tuned model and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1c8c77b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "checkpoint = torch.load(\"model_checkpoints/Gaussian_0.1_4_inf_model.pt\")\n",
    "\n",
    "# Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "#checkpoint['model_state_dict'][\"input_resolution\"] = model.input_resolution #default is 224\n",
    "#checkpoint['model_state_dict'][\"context_length\"] = model.context_length # default is 77\n",
    "#checkpoint['model_state_dict'][\"vocab_size\"] = model.vocab_size \n",
    "\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7e12b378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                                             | 1/1000 [00:01<21:08,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 0, Percent: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████████▏                                                                                                                             | 101/1000 [02:06<18:50,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 43, Percent: 42.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████████████████▏                                                                                                               | 201/1000 [04:16<16:46,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 92, Percent: 45.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████████████████████████████████████▏                                                                                                 | 301/1000 [06:21<14:39,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 136, Percent: 45.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████▏                                                                                   | 401/1000 [08:27<12:32,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 183, Percent: 45.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████████████████████████████████████████████████████████▉                                                                                   | 407/1000 [08:36<12:31,  1.27s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m pgd_df \u001b[38;5;241m=\u001b[39m make_adverserial_inference_df(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/pgdAttack\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtop_k_inf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgd_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpgd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36mtop_k_inf\u001b[0;34m(model, df, k, attack_mode)\u001b[0m\n\u001b[1;32m     19\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m image_input \u001b[38;5;241m=\u001b[39m preprocess(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 22\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[43mfind_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcaption\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m matches:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m match: \n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mfind_match\u001b[0;34m(image_input, text_inputs, model, classes, k, print_only)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      3\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_image(image_input)\n\u001b[0;32m----> 4\u001b[0m     text_features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Pick the top 5 most similar labels for the image\u001b[39;00m\n\u001b[1;32m      7\u001b[0m image_features \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/openaiclip/lib/python3.8/site-packages/clip/model.py:348\u001b[0m, in \u001b[0;36mCLIP.encode_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    346\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    347\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    350\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_final(x)\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/openaiclip/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/anaconda3/envs/openaiclip/lib/python3.8/site-packages/clip/model.py:203\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/openaiclip/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/anaconda3/envs/openaiclip/lib/python3.8/site-packages/torch/nn/modules/container.py:117\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/openaiclip/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/anaconda3/envs/openaiclip/lib/python3.8/site-packages/clip/model.py:190\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 190\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/openaiclip/lib/python3.8/site-packages/clip/model.py:187\u001b[0m, in \u001b[0;36mResidualAttentionBlock.attention\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattention\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_mask\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/openaiclip/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/anaconda3/envs/openaiclip/lib/python3.8/site-packages/torch/nn/modules/activation.py:978\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m    968\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m    969\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    975\u001b[0m         q_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj_weight, k_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj_weight,\n\u001b[1;32m    976\u001b[0m         v_proj_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj_weight)\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/openaiclip/lib/python3.8/site-packages/torch/nn/functional.py:4144\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4141\u001b[0m scaling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(head_dim) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m   4143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[0;32m-> 4144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(key, value):\n\u001b[1;32m   4145\u001b[0m         \u001b[38;5;66;03m# self-attention\u001b[39;00m\n\u001b[1;32m   4146\u001b[0m         q, k, v \u001b[38;5;241m=\u001b[39m linear(query, in_proj_weight, in_proj_bias)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   4148\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(key, value):\n\u001b[1;32m   4149\u001b[0m         \u001b[38;5;66;03m# encoder-decoder attention\u001b[39;00m\n\u001b[1;32m   4150\u001b[0m         \u001b[38;5;66;03m# This is inline in_proj function with in_proj_weight and in_proj_bias\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pgd_df = make_adverserial_inference_df(\"data/pgdAttack\")\n",
    "top_k_inf(model, pgd_df, k=5, attack_mode='pgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e468e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                                              | 1/997 [00:01<21:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 1, Percent: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████████▎                                                                                                                              | 101/997 [02:06<18:42,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct raw: 42, Percent: 41.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|███████████████▊                                                                                                                             | 112/997 [02:21<18:40,  1.27s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m fgsm_df \u001b[38;5;241m=\u001b[39m make_adverserial_inference_df(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/attack\u001b[39m\u001b[38;5;124m\"\u001b[39m, fgsm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtop_k_inf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfgsm_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfgsm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36mtop_k_inf\u001b[0;34m(model, df, k, attack_mode)\u001b[0m\n\u001b[1;32m     19\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m image_input \u001b[38;5;241m=\u001b[39m preprocess(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 22\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[43mfind_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcaption\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m matches:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m match: \n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mfind_match\u001b[0;34m(image_input, text_inputs, model, classes, k, print_only)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      3\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_image(image_input)\n\u001b[0;32m----> 4\u001b[0m     text_features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Pick the top 5 most similar labels for the image\u001b[39;00m\n\u001b[1;32m      7\u001b[0m image_features \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/openaiclip/lib/python3.8/site-packages/clip/model.py:354\u001b[0m, in \u001b[0;36mCLIP.encode_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    350\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_final(x)\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# x.shape = [batch_size, n_ctx, transformer.width]\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# take features from the eot embedding (eot_token is the highest number in each sequence)\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_projection\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fgsm_df = make_adverserial_inference_df(\"data/attack\", fgsm=True)\n",
    "top_k_inf(model, fgsm_df, k=5, attack_mode='fgsm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc334a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:openaiclip]",
   "language": "python",
   "name": "conda-env-openaiclip-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
